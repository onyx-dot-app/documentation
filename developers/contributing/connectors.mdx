---
title: "Connectors"
description: "Guide for developing data connectors in Onyx"
icon: "plug"
---

# Connectors

Connectors in Onyx are integrations that pull data from external sources, process it, and make it searchable within the platform. This guide covers how to develop, test, and contribute new connectors.

## What are Connectors?

Connectors enable Onyx to ingest data from various sources:
- **File systems** (local files, network drives)
- **Cloud storage** (Google Drive, SharePoint, S3, etc.)
- **Documentation platforms** (Confluence, Notion, GitBook, etc.)
- **Communication tools** (Slack, Discord, Teams, etc.)
- **Project management** (Jira, Linear, Asana, etc.)
- **Code repositories** (GitHub, GitLab, etc.)
- **Databases** (PostgreSQL, MySQL, etc.)

## Connector Architecture

### Core Components

```python
from abc import ABC, abstractmethod
from onyx.connectors.models import Document, Section
from onyx.connectors.interfaces import LoadConnector, PollConnector, EventConnector

class BaseConnector(LoadConnector, ABC):
    def __init__(self, batch_size: int = 10):
        self.batch_size = batch_size
    
    @abstractmethod
    def load_credentials(self, credentials: dict[str, Any]) -> dict[str, Any]:
        """Load and validate credentials"""
        pass
    
    @abstractmethod 
    def load_from_state(self) -> GenerateDocumentsOutput:
        """Load documents from the source"""
        pass
    
    @abstractmethod
    def poll_source(self, start: SecondsSinceUnixEpoch, end: SecondsSinceUnixEpoch) -> GenerateDocumentsOutput:
        """Poll for changes in the source"""
        pass

class Document:
    """Represents a document from an external source"""
    id: str                          # Unique identifier
    sections: list[Section]          # Document content sections
    source: DocumentSource           # Source type
    semantic_identifier: str         # Human-readable identifier
    doc_updated_at: datetime         # Last modified time
    primary_owners: list[str]        # Document owners
    secondary_owners: list[str]      # Additional owners
    metadata: dict[str, Any]         # Additional metadata
```

### Connector Types

**Load Connectors**: One-time data import
```python
class StaticFileConnector(LoadConnector):
    def load_from_state(self) -> GenerateDocumentsOutput:
        # Import all files once
        for file_path in self.get_file_paths():
            yield self.process_file(file_path)
```

**Poll Connectors**: Regular updates
```python
class APIConnector(PollConnector):
    def poll_source(self, start: SecondsSinceUnixEpoch, end: SecondsSinceUnixEpoch) -> GenerateDocumentsOutput:
        # Check for updates since last poll
        updated_docs = self.api_client.get_updated_documents(since=start)
        for doc in updated_docs:
            yield self.convert_to_document(doc)
```

**Event Connectors**: Real-time updates
```python
class WebhookConnector(EventConnector):
    def handle_event(self, event: dict) -> GenerateDocumentsOutput:
        # Process webhook events
        if event["type"] == "document_updated":
            doc = self.fetch_document(event["document_id"])
            yield self.convert_to_document(doc)
```

## Creating a New Connector

### 1. Define the Connector Class

Create a new file in `backend/onyx/connectors/`:

```python
import requests
from datetime import datetime
from typing import Any, Dict, List, Optional

from onyx.connectors.interfaces import LoadConnector, PollConnector
from onyx.connectors.models import Document, Section, DocumentSource
from onyx.utils.logger import setup_logger

logger = setup_logger()

class ExampleAPIConnector(LoadConnector, PollConnector):
    def __init__(
        self,
        api_base_url: str,
        batch_size: int = 10
    ):
        self.api_base_url = api_base_url.rstrip('/')
        self.batch_size = batch_size
        self.api_client = None
    
    def load_credentials(self, credentials: dict[str, Any]) -> dict[str, Any]:
        """Validate and load API credentials"""
        api_key = credentials.get("api_key")
        if not api_key:
            raise ValueError("API key is required")
        
        # Test the credentials
        self.api_client = APIClient(self.api_base_url, api_key)
        try:
            self.api_client.test_connection()
        except Exception as e:
            raise ValueError(f"Invalid credentials: {str(e)}")
        
        return {"api_key": api_key}
    
    def load_from_state(self) -> GenerateDocumentsOutput:
        """Load all documents from the API"""
        logger.info("Starting full document load")
        
        try:
            documents = self.api_client.get_all_documents()
            
            for doc_batch in self._batch_documents(documents):
                converted_docs = []
                for doc in doc_batch:
                    try:
                        converted_doc = self._convert_api_doc_to_document(doc)
                        converted_docs.append(converted_doc)
                    except Exception as e:
                        logger.error(f"Error converting document {doc.get('id', 'unknown')}: {e}")
                
                if converted_docs:
                    yield converted_docs
        
        except Exception as e:
            logger.error(f"Error during document load: {e}")
            raise
    
    def poll_source(
        self, 
        start: SecondsSinceUnixEpoch, 
        end: SecondsSinceUnixEpoch
    ) -> GenerateDocumentsOutput:
        """Poll for document updates"""
        logger.info(f"Polling for updates between {start} and {end}")
        
        try:
            updated_docs = self.api_client.get_updated_documents(
                since=datetime.fromtimestamp(start),
                until=datetime.fromtimestamp(end)
            )
            
            for doc_batch in self._batch_documents(updated_docs):
                converted_docs = []
                for doc in doc_batch:
                    try:
                        converted_doc = self._convert_api_doc_to_document(doc)
                        converted_docs.append(converted_doc)
                    except Exception as e:
                        logger.error(f"Error converting document {doc.get('id', 'unknown')}: {e}")
                
                if converted_docs:
                    yield converted_docs
        
        except Exception as e:
            logger.error(f"Error during polling: {e}")
            raise
    
    def _convert_api_doc_to_document(self, api_doc: dict) -> Document:
        """Convert API document to Onyx Document format"""
        
        # Extract content sections
        sections = []
        if api_doc.get("content"):
            sections.append(Section(
                link=api_doc.get("url", ""),
                text=api_doc["content"]
            ))
        
        # Additional sections from API
        for section_data in api_doc.get("sections", []):
            sections.append(Section(
                link=section_data.get("url", ""),
                text=section_data.get("text", "")
            ))
        
        # Extract metadata
        metadata = {
            "source_type": "example_api",
            "category": api_doc.get("category"),
            "tags": api_doc.get("tags", []),
            "custom_field": api_doc.get("custom_field")
        }
        
        return Document(
            id=api_doc["id"],
            sections=sections,
            source=DocumentSource.EXAMPLE_API,
            semantic_identifier=api_doc.get("title", api_doc["id"]),
            doc_updated_at=self._parse_datetime(api_doc.get("updated_at")),
            primary_owners=[api_doc.get("author", "")],
            secondary_owners=api_doc.get("collaborators", []),
            metadata=metadata
        )
    
    def _batch_documents(self, documents: List[dict]) -> Iterator[List[dict]]:
        """Batch documents for processing"""
        for i in range(0, len(documents), self.batch_size):
            yield documents[i:i + self.batch_size]
    
    def _parse_datetime(self, date_str: Optional[str]) -> Optional[datetime]:
        """Parse datetime string from API"""
        if not date_str:
            return None
        
        try:
            return datetime.fromisoformat(date_str.replace('Z', '+00:00'))
        except ValueError:
            logger.warning(f"Could not parse datetime: {date_str}")
            return None

class APIClient:
    """HTTP client for the external API"""
    
    def __init__(self, base_url: str, api_key: str):
        self.base_url = base_url
        self.api_key = api_key
        self.session = requests.Session()
        self.session.headers.update({
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        })
    
    def test_connection(self) -> bool:
        """Test API connection and credentials"""
        response = self.session.get(f"{self.base_url}/api/v1/me")
        response.raise_for_status()
        return True
    
    def get_all_documents(self) -> List[dict]:
        """Fetch all documents from the API"""
        documents = []
        page = 1
        
        while True:
            response = self.session.get(
                f"{self.base_url}/api/v1/documents",
                params={"page": page, "per_page": 100}
            )
            response.raise_for_status()
            data = response.json()
            
            documents.extend(data.get("documents", []))
            
            if not data.get("has_more", False):
                break
            
            page += 1
        
        return documents
    
    def get_updated_documents(
        self, 
        since: datetime, 
        until: datetime
    ) -> List[dict]:
        """Fetch documents updated in time range"""
        response = self.session.get(
            f"{self.base_url}/api/v1/documents",
            params={
                "updated_since": since.isoformat(),
                "updated_until": until.isoformat(),
                "per_page": 1000
            }
        )
        response.raise_for_status()
        return response.json().get("documents", [])
```

### 2. Register the Connector

Add your connector to the factory in `backend/onyx/connectors/factory.py`:

```python
from onyx.connectors.example_api.connector import ExampleAPIConnector

CONNECTOR_MAP = {
    # ... existing connectors
    DocumentSource.EXAMPLE_API: ExampleAPIConnector,
}
```

### 3. Add Document Source

Add the new source type to `backend/onyx/connectors/models.py`:

```python
class DocumentSource(str, Enum):
    # ... existing sources
    EXAMPLE_API = "example_api"
```

### 4. Create Configuration Schema

Define the connector configuration in `backend/onyx/connectors/example_api/`:

```python
# config.py
from pydantic import BaseModel, Field
from typing import Optional

class ExampleAPIConfig(BaseModel):
    api_base_url: str = Field(
        description="Base URL for the Example API"
    )
    batch_size: int = Field(
        default=10,
        description="Number of documents to process in each batch"
    )
    include_archived: bool = Field(
        default=False,
        description="Whether to include archived documents"
    )

class ExampleAPICredentials(BaseModel):
    api_key: str = Field(
        description="API key for authentication"
    )
    api_secret: Optional[str] = Field(
        default=None,
        description="API secret (if required)"
    )
```

## Testing Connectors

### Unit Tests

Create comprehensive tests in `backend/tests/unit/connectors/test_example_api.py`:

```python
import pytest
from unittest.mock import Mock, patch
from datetime import datetime

from onyx.connectors.example_api.connector import ExampleAPIConnector
from onyx.connectors.models import DocumentSource

class TestExampleAPIConnector:
    @pytest.fixture
    def connector(self):
        return ExampleAPIConnector(
            api_base_url="https://api.example.com",
            batch_size=5
        )
    
    @pytest.fixture
    def mock_credentials(self):
        return {"api_key": "test_key_123"}
    
    def test_load_credentials_success(self, connector, mock_credentials):
        with patch.object(connector, '_create_api_client') as mock_create:
            mock_client = Mock()
            mock_client.test_connection.return_value = True
            mock_create.return_value = mock_client
            
            result = connector.load_credentials(mock_credentials)
            assert result == mock_credentials
    
    def test_load_credentials_missing_key(self, connector):
        with pytest.raises(ValueError, match="API key is required"):
            connector.load_credentials({})
    
    def test_load_credentials_invalid_key(self, connector):
        with patch.object(connector, '_create_api_client') as mock_create:
            mock_client = Mock()
            mock_client.test_connection.side_effect = Exception("Invalid API key")
            mock_create.return_value = mock_client
            
            with pytest.raises(ValueError, match="Invalid credentials"):
                connector.load_credentials({"api_key": "invalid"})
    
    def test_convert_api_doc_to_document(self, connector):
        api_doc = {
            "id": "doc_123",
            "title": "Test Document",
            "content": "This is test content",
            "updated_at": "2023-01-01T12:00:00Z",
            "author": "test@example.com",
            "url": "https://example.com/doc/123"
        }
        
        doc = connector._convert_api_doc_to_document(api_doc)
        
        assert doc.id == "doc_123"
        assert doc.semantic_identifier == "Test Document"
        assert doc.source == DocumentSource.EXAMPLE_API
        assert len(doc.sections) == 1
        assert doc.sections[0].text == "This is test content"
        assert doc.primary_owners == ["test@example.com"]
    
    def test_batch_documents(self, connector):
        documents = [{"id": f"doc_{i}"} for i in range(12)]
        batches = list(connector._batch_documents(documents))
        
        assert len(batches) == 3  # 12 docs with batch_size=5 -> 3 batches
        assert len(batches[0]) == 5
        assert len(batches[1]) == 5
        assert len(batches[2]) == 2
    
    @patch('onyx.connectors.example_api.connector.APIClient')
    def test_load_from_state(self, mock_api_client_class, connector, mock_credentials):
        # Setup
        connector.load_credentials(mock_credentials)
        mock_client = mock_api_client_class.return_value
        mock_client.get_all_documents.return_value = [
            {"id": "doc_1", "title": "Doc 1", "content": "Content 1"},
            {"id": "doc_2", "title": "Doc 2", "content": "Content 2"}
        ]
        
        # Execute
        results = list(connector.load_from_state())
        
        # Verify
        assert len(results) == 1  # One batch
        assert len(results[0]) == 2  # Two documents
        assert results[0][0].id == "doc_1"
        assert results[0][1].id == "doc_2"
```

### Integration Tests

Test with real API (using test environment):

```python
@pytest.mark.integration
class TestExampleAPIConnectorIntegration:
    @pytest.fixture
    def real_connector(self):
        return ExampleAPIConnector(
            api_base_url=os.getenv("TEST_API_URL"),
            batch_size=3
        )
    
    @pytest.fixture
    def real_credentials(self):
        return {"api_key": os.getenv("TEST_API_KEY")}
    
    def test_real_api_connection(self, real_connector, real_credentials):
        # Skip if no test credentials
        if not all([os.getenv("TEST_API_URL"), os.getenv("TEST_API_KEY")]):
            pytest.skip("Integration test credentials not provided")
        
        # Test credential loading
        result = real_connector.load_credentials(real_credentials)
        assert result == real_credentials
    
    def test_load_real_documents(self, real_connector, real_credentials):
        if not all([os.getenv("TEST_API_URL"), os.getenv("TEST_API_KEY")]):
            pytest.skip("Integration test credentials not provided")
        
        real_connector.load_credentials(real_credentials)
        
        # Load documents
        results = list(real_connector.load_from_state())
        
        # Verify structure
        assert len(results) > 0
        for batch in results:
            assert isinstance(batch, list)
            for doc in batch:
                assert hasattr(doc, 'id')
                assert hasattr(doc, 'sections')
                assert hasattr(doc, 'source')
```

## Connector Best Practices

### Error Handling

```python
def robust_api_call(self, endpoint: str, **kwargs):
    """Make API call with proper error handling"""
    max_retries = 3
    retry_delay = 1
    
    for attempt in range(max_retries):
        try:
            response = self.session.get(f"{self.base_url}/{endpoint}", **kwargs)
            
            # Handle rate limiting
            if response.status_code == 429:
                retry_after = int(response.headers.get('Retry-After', retry_delay))
                logger.warning(f"Rate limited, waiting {retry_after}s")
                time.sleep(retry_after)
                continue
            
            # Handle other HTTP errors
            response.raise_for_status()
            return response.json()
            
        except requests.RequestException as e:
            if attempt == max_retries - 1:
                raise ConnectorError(f"API call failed after {max_retries} attempts: {e}")
            
            logger.warning(f"API call attempt {attempt + 1} failed: {e}")
            time.sleep(retry_delay * (2 ** attempt))  # Exponential backoff
```

### Rate Limiting

```python
import time
from datetime import datetime, timedelta

class RateLimiter:
    def __init__(self, calls_per_minute: int = 60):
        self.calls_per_minute = calls_per_minute
        self.calls = []
    
    def wait_if_needed(self):
        """Wait if we're approaching rate limits"""
        now = datetime.now()
        # Remove calls older than 1 minute
        self.calls = [call_time for call_time in self.calls 
                     if now - call_time < timedelta(minutes=1)]
        
        if len(self.calls) >= self.calls_per_minute:
            # Wait until the oldest call is more than 1 minute old
            sleep_time = 60 - (now - self.calls[0]).total_seconds()
            if sleep_time > 0:
                logger.info(f"Rate limiting: sleeping for {sleep_time:.1f}s")
                time.sleep(sleep_time)
        
        self.calls.append(now)
```

### Memory Management

```python
def process_large_dataset(self, dataset_id: str) -> GenerateDocumentsOutput:
    """Process large datasets efficiently"""
    
    # Stream data instead of loading everything into memory
    for page in self.api_client.stream_documents(dataset_id):
        batch = []
        
        for doc_data in page:
            try:
                doc = self._convert_api_doc_to_document(doc_data)
                batch.append(doc)
                
                # Yield batches to prevent memory issues
                if len(batch) >= self.batch_size:
                    yield batch
                    batch = []
                    
            except Exception as e:
                logger.error(f"Failed to process document: {e}")
        
        # Yield remaining documents
        if batch:
            yield batch
```

### Configuration Validation

```python
def validate_config(self) -> None:
    """Validate connector configuration"""
    
    # Validate required fields
    if not self.api_base_url:
        raise ValueError("API base URL is required")
    
    # Validate URL format
    if not self.api_base_url.startswith(('http://', 'https://')):
        raise ValueError("API base URL must include protocol (http:// or https://)")
    
    # Validate batch size
    if self.batch_size <= 0 or self.batch_size > 1000:
        raise ValueError("Batch size must be between 1 and 1000")
    
    # Test API connectivity
    try:
        self.api_client.test_connection()
    except Exception as e:
        raise ValueError(f"Cannot connect to API: {e}")
```

## Advanced Connector Features

### Incremental Sync

```python
def get_incremental_updates(self, last_sync: datetime) -> GenerateDocumentsOutput:
    """Get only documents updated since last sync"""
    
    # Use API's timestamp filtering
    updated_docs = self.api_client.get_documents_since(last_sync)
    
    # Also check for deleted documents
    deleted_doc_ids = self.api_client.get_deleted_documents_since(last_sync)
    
    # Yield updated documents
    for doc_batch in self._batch_documents(updated_docs):
        yield [self._convert_api_doc_to_document(doc) for doc in doc_batch]
    
    # Yield deletion markers
    if deleted_doc_ids:
        for doc_id in deleted_doc_ids:
            yield [Document(
                id=doc_id,
                sections=[],
                source=self.source_type,
                semantic_identifier=doc_id,
                metadata={"_deleted": True}
            )]
```

### Permission Handling

```python
def filter_accessible_documents(self, documents: List[dict]) -> List[dict]:
    """Filter documents based on user permissions"""
    
    accessible_docs = []
    
    for doc in documents:
        try:
            # Check if current user can access this document
            if self.api_client.check_document_access(doc["id"]):
                accessible_docs.append(doc)
            else:
                logger.debug(f"Skipping document {doc['id']} - no access")
        except Exception as e:
            logger.error(f"Error checking access for document {doc['id']}: {e}")
    
    return accessible_docs
```

### Content Processing

```python
def process_document_content(self, raw_content: str, content_type: str) -> List[Section]:
    """Process and clean document content"""
    
    sections = []
    
    if content_type == "markdown":
        # Parse markdown sections
        sections = self._parse_markdown_sections(raw_content)
    elif content_type == "html":
        # Extract text from HTML
        sections = self._parse_html_sections(raw_content)
    elif content_type == "pdf":
        # Extract text from PDF
        sections = self._parse_pdf_sections(raw_content)
    else:
        # Plain text
        sections = [Section(text=raw_content, link="")]
    
    # Clean and validate sections
    return [section for section in sections if section.text.strip()]

def _parse_markdown_sections(self, markdown: str) -> List[Section]:
    """Parse markdown into logical sections"""
    import re
    
    sections = []
    current_section = ""
    current_heading = ""
    
    for line in markdown.split('\n'):
        if re.match(r'^#{1,6}\s', line):  # Heading
            if current_section.strip():
                sections.append(Section(
                    text=f"{current_heading}\n{current_section}".strip(),
                    link=""
                ))
                current_section = ""
            current_heading = line
        else:
            current_section += line + '\n'
    
    # Add final section
    if current_section.strip():
        sections.append(Section(
            text=f"{current_heading}\n{current_section}".strip(),
            link=""
        ))
    
    return sections
```

## Contributing Your Connector

### 1. Documentation

Create comprehensive documentation:

```markdown
# Example API Connector

## Overview
The Example API Connector integrates with Example API to import documents into Onyx.

## Setup
1. Obtain an API key from Example API
2. Configure the connector with your API credentials
3. Select document types to import

## Configuration
- `api_base_url`: Base URL for the API
- `batch_size`: Number of documents to process at once
- `include_archived`: Whether to include archived documents

## Limitations
- Rate limited to 100 requests per minute
- Cannot access private documents without proper permissions
- Large files may take longer to process
```

### 2. Testing

Ensure comprehensive test coverage:
- Unit tests for all methods
- Integration tests with real API
- Error handling tests
- Performance tests for large datasets

### 3. Pull Request

1. Fork the repository
2. Create a feature branch: `git checkout -b connector/example-api`
3. Add connector code and tests
4. Update documentation
5. Submit pull request with:
   - Description of the connector
   - Setup instructions
   - Testing results
   - Example configurations

## Connector Maintenance

### Monitoring

```python
class ConnectorMetrics:
    def __init__(self, connector_name: str):
        self.connector_name = connector_name
        self.metrics = {
            "documents_processed": 0,
            "errors": 0,
            "last_sync": None,
            "processing_time": 0
        }
    
    def record_success(self, doc_count: int, processing_time: float):
        self.metrics["documents_processed"] += doc_count
        self.metrics["processing_time"] += processing_time
        self.metrics["last_sync"] = datetime.now()
    
    def record_error(self, error: Exception):
        self.metrics["errors"] += 1
        logger.error(f"Connector {self.connector_name} error: {error}")
```

### Updates and Versioning

```python
class ConnectorVersion:
    CURRENT_VERSION = "1.2.0"
    MIN_SUPPORTED_API_VERSION = "2.0"
    
    @classmethod
    def check_api_compatibility(cls, api_version: str) -> bool:
        """Check if API version is supported"""
        from packaging import version
        return version.parse(api_version) >= version.parse(cls.MIN_SUPPORTED_API_VERSION)
```

## Getting Help

- Check existing connectors in `backend/onyx/connectors/` for examples
- Review the [Files and Connectors API documentation](/api_reference/files_connectors/overview)
- Join our [Slack community](https://join.slack.com/t/onyx-dot-app/shared_invite/zt-34lu4m7xg-TsKGO6h8PDvR5W27zTdyhA) for help
- Open GitHub issues for bugs or connector requests
- Contribute to connector templates and documentation 