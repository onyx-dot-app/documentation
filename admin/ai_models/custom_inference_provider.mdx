---
title: "Custom Inference Provider"
description: "Custom Inference Provider"
icon: "server"
---

import NavigateToAiModels from "/snippets/navigate-to-ai-models.mdx";
import StandardAiModelConfigs from "/snippets/standard-ai-model-configs.mdx";

## Guide

If you want to use an AI provider that is not supported by Onyx, you can create a custom inference provider.

<Tip>
  Your custom provider must provide OpenAI-compatible API endpoints.
</Tip>

<Steps>
  <Step title="Setup your Custom Inference Provider">
    Determine your provider's API base URL. It should look something like `https://yourprovider.com/v1`.
  </Step>

  <NavigateToAiModels />

  <Step title="Configure Custom Inference Provider">
    Select **Add Custom LLM Provider** from the available providers.

    Give your provider a **Display Name**.

    Enter your model's **Provider Name**.

    <Note>
      The **Provider Name** must match Litellm's [list of supported providers](https://docs.litellm.ai/docs/providers).
    </Note>

    Enter the rest of the optional fields if relevant.

    Enter each of the models you want to use with your provider.
  </Step>

  <StandardAiModelConfigs />
</Steps>
