---
title: LLM Configuration Overview
description: "Configure Large Language Models for Onyx"
icon: "brain"
---

Configure the Large Language Models (LLMs) that power Onyx's AI capabilities. Onyx supports a wide range of hosting services and local deployments, giving you flexibility in choosing the right model for your needs.

## Understanding LLMs in Onyx

### **What are LLMs used for?**
Large Language Models in Onyx serve as the core intelligence engine that:
- **Interprets retrieved documents** from your knowledge base search results
- **Extracts useful knowledge** from the most relevant document sections
- **Generates AI answers** by synthesizing information from multiple sources
- **Powers chat conversations** with contextual understanding
- **Enables agents** to perform complex reasoning and task completion

### **Default Recommendations**
For the best performance and quality, we recommend:
- **GPT-4 or GPT-4o** from OpenAI - Most powerful and reliable
- **Claude 3.5 Sonnet** from Anthropic - Excellent reasoning and analysis
- **Azure OpenAI** (GPT-4) - Enterprise-grade with data residency options
- **Llama 3.1 70B/405B** (self-hosted) - Open-source alternative with strong performance

## Supported LLM Providers

Onyx supports an extensive range of LLM providers through the [LiteLLM library](https://python.langchain.com/docs/integrations/chat/litellm):

### **Cloud Providers**
- **[OpenAI](../llm_providers/openai)** - GPT-4, GPT-4o, GPT-3.5-turbo
- **[Anthropic](../llm_providers/anthropic)** - Claude 3.5 Sonnet, Claude 3 Opus/Haiku
- **[Azure OpenAI](../llm_providers/azure_openai)** - Enterprise-grade GPT models
- **[Google Vertex AI](../llm_providers/vertex_ai)** - Gemini Pro, PaLM 2
- **AWS Bedrock** - Access to multiple model providers
- **HuggingFace** - Various open-source models
- **Cohere** - Command models
- **Replicate** - Community-hosted models

### **Self-Hosted Options**
- **[Ollama](../llm_providers/ollama)** - Local model hosting with easy setup
- **GPT4All** - Offline-capable models
- **[Custom Model Servers](../llm_providers/custom)** - Any service following OpenAI API format
- **FastChat** - Multi-model serving framework

## Configuration Process

### **Access LLM Configuration**
1. Navigate to the **Admin Dashboard**
2. Click on the **LLM** tab in the sidebar
3. You'll see options to configure multiple LLM providers simultaneously

### **Multiple Provider Support**
One of Onyx's key features is **multi-provider support**:
- Configure multiple LLM providers at once
- Assign different models to different agents
- Play to each model's strengths for specific use cases
- A/B test model performance across your organization

## Choosing the Right Model

### **Performance vs Cost Considerations**

| Provider | Performance | Cost | Data Privacy | Setup Complexity |
|----------|-------------|------|--------------|------------------|
| OpenAI GPT-4 | Excellent | High | Medium | Simple |
| Claude 3.5 Sonnet | Excellent | High | Medium | Simple |
| Azure OpenAI | Excellent | Very High | High | Moderate |
| Vertex AI | Very Good | Moderate | High | Complex |
| Ollama (Llama 3.1 70B) | Good | Low | Excellent | Very Complex |

### **Use Case Recommendations**

**Enterprise Production**
- Azure OpenAI or AWS Bedrock for compliance
- Claude 3.5 Sonnet for complex reasoning
- Multiple providers for redundancy

**Cost-Conscious Deployments**
- GPT-4o for balanced performance/cost
- Self-hosted Llama models for high volume
- HuggingFace smaller models for basic tasks

**Maximum Privacy**
- Self-hosted Ollama with local models
- Custom model servers in your infrastructure
- Avoid cloud providers entirely

**High Performance**
- GPT-4 or Claude 3.5 Sonnet for quality
- Multiple providers for load balancing
- Dedicated model endpoints for speed

## Best Practices

### **Security Considerations**
- **API Key Management**: Store keys securely, rotate regularly
- **Network Security**: Use VPNs or private endpoints when possible
- **Data Retention**: Understand each provider's data retention policies
- **Access Control**: Limit API key access to necessary personnel

### **Performance Optimization**
- **Model Selection**: Choose appropriate models for each use case
- **Timeout Configuration**: Set realistic timeouts for your models
- **Caching**: Enable response caching where appropriate
- **Load Balancing**: Distribute requests across multiple providers

### **Cost Management**
- **Usage Monitoring**: Track API usage and costs regularly
- **Model Efficiency**: Use smaller models for simpler tasks
- **Caching**: Reduce redundant API calls through intelligent caching
- **Budget Alerts**: Set up cost monitoring and alerts

### **Reliability**
- **Multi-Provider Setup**: Configure backup providers for redundancy
- **Health Monitoring**: Monitor model endpoint availability
- **Graceful Degradation**: Handle provider outages gracefully
- **Testing**: Regularly test all configured providers

## Troubleshooting Common Issues

### **Connection Problems**
- Verify API keys are correct and have sufficient permissions
- Check network connectivity to provider endpoints
- Ensure firewall rules allow outbound connections
- Validate SSL/TLS certificate chains

### **Performance Issues**
- Increase timeout values for slower models
- Check provider-specific rate limits
- Monitor model endpoint health
- Consider switching to faster models for time-sensitive tasks

### **Quality Problems**
- Review and adjust system prompts
- Test different models for your specific use case
- Fine-tune prompt engineering for better results
- Consider using more powerful models for complex queries

### **Cost Overruns**
- Implement usage monitoring and alerts
- Optimize prompt lengths to reduce token usage
- Use model-appropriate selection (don't use GPT-4 for simple tasks)
- Implement request deduplication and caching

## Provider Setup Guides

For detailed setup instructions, see our provider-specific guides:

- **[OpenAI Configuration](../llm_providers/openai)** - Simple API key setup for GPT models
- **[Anthropic Configuration](../llm_providers/anthropic)** - Quick setup for Claude models
- **[Azure OpenAI Configuration](../llm_providers/azure_openai)** - Enterprise-grade GPT with detailed setup
- **[Google Vertex AI Configuration](../llm_providers/vertex_ai)** - Two authentication methods with comprehensive guide
- **[Ollama Configuration](../llm_providers/ollama)** - Self-hosted models with optimization tips
- **[Custom Model Servers](../llm_providers/custom)** - Integrate any OpenAI-compatible API

## Getting Help

For additional support with LLM configuration:
- Join our [Slack Community](https://join.slack.com/t/onyx-dot-app/shared_invite/zt-34lu4m7xg-TsKGO6h8PDvR5W27zTdyhA) for community support
- Review the [GitHub repository](https://github.com/onyx-dot-app/onyx) for technical details
- Contact the Onyx team for enterprise support options

The flexibility of Onyx's LLM system allows you to start with simple cloud providers and evolve to more complex, privacy-focused solutions as your needs grow.