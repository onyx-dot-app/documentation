---
title: 'Self-Hosted'
description: 'Configure self-hosted LLM models for use with Onyx'
---

# Self-Hosted LLM Configuration

Self-hosted LLMs allow you to run AI models on your own infrastructure, providing complete control over data, security, and costs. This guide will help you configure self-hosted models in Onyx.

## Overview

Self-hosted LLMs offer several advantages for enterprise deployments:

- **Data Privacy**: Complete control over data processing and storage
- **Customization**: Ability to fine-tune models for specific use cases
- **Cost Control**: Predictable costs without per-token pricing
- **Compliance**: Meet regulatory requirements for data sovereignty
- **Offline Operation**: Function without internet connectivity

### Supported Model Types

- **Open Source Models**: Llama 2, Mistral, CodeLlama, and others
- **Fine-tuned Models**: Custom models trained on your data
- **Quantized Models**: Optimized models for reduced resource usage
- **Multi-modal Models**: Models supporting text, image, and other inputs

## Prerequisites

Before configuring self-hosted LLMs in Onyx, ensure you have:

1. **Infrastructure**: Sufficient compute resources (GPU/CPU)
2. **Model Files**: Downloaded model weights and configuration
3. **Inference Server**: Ollama, vLLM, or similar inference engine
4. **Network Access**: Proper network configuration for Onyx to reach the model
5. **Monitoring**: Tools to monitor model performance and resource usage

## Deployment Options

### Option 1: Ollama (Recommended for Development)

Ollama provides a simple way to run open-source models locally.

#### Step 1: Install Ollama

```bash
# macOS
curl -fsSL https://ollama.ai/install.sh | sh

# Linux
curl -fsSL https://ollama.ai/install.sh | sh

# Windows
# Download from https://ollama.ai/download
```

#### Step 2: Pull and Run Models

```bash
# Pull a model
ollama pull llama2:7b

# Run the model
ollama run llama2:7b
```

#### Step 3: Configure in Onyx

1. Navigate to the Onyx admin panel
2. Go to **LLM Config** > **Self-Hosted**
3. Select **Ollama** as the inference server
4. Enter the Ollama server URL (default: `http://localhost:11434`)
5. Choose the model name (e.g., `llama2:7b`)
6. Configure additional parameters

### Option 2: vLLM (Recommended for Production)

vLLM provides high-performance inference with advanced features.

#### Step 1: Install vLLM

```bash
pip install vllm
```

#### Step 2: Start the Server

```bash
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-2-7b-chat-hf \
    --host 0.0.0.0 \
    --port 8000
```

#### Step 3: Configure in Onyx

1. Navigate to the Onyx admin panel
2. Go to **LLM Config** > **Self-Hosted**
3. Select **vLLM** as the inference server
4. Enter the vLLM server URL (e.g., `http://localhost:8000`)
5. Choose the model name
6. Configure additional parameters

### Option 3: Custom Inference Server

For advanced deployments, you can use custom inference servers.

#### Step 1: Set Up Custom Server

Implement a server that exposes the OpenAI-compatible API:

```python
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI()

class ChatRequest(BaseModel):
    model: str
    messages: list
    temperature: float = 0.7
    max_tokens: int = 1000

@app.post("/v1/chat/completions")
async def chat_completions(request: ChatRequest):
    # Your custom inference logic here
    response = await generate_response(request)
    return response
```

#### Step 2: Configure in Onyx

1. Navigate to the Onyx admin panel
2. Go to **LLM Config** > **Self-Hosted**
3. Select **Custom** as the inference server
4. Enter your custom server URL
5. Configure the API format and parameters

## Model Configuration

### Model Selection

Choose the appropriate model based on your requirements:

#### General Purpose Models
- **Llama 2 7B**: Good balance of performance and resource usage
- **Llama 2 13B**: Better performance with higher resource requirements
- **Mistral 7B**: Strong performance for reasoning tasks
- **CodeLlama**: Specialized for code generation and analysis

#### Specialized Models
- **Llama 2 Chat**: Optimized for conversational AI
- **Mistral Instruct**: Fine-tuned for instruction following
- **Phi-2**: Small but capable model for edge deployment

### Resource Requirements

Ensure your infrastructure meets the model requirements:

#### GPU Requirements
- **7B Models**: 8-16GB VRAM
- **13B Models**: 16-32GB VRAM
- **70B Models**: 80GB+ VRAM (multi-GPU)

#### CPU Requirements
- **7B Models**: 16+ CPU cores, 32GB RAM
- **13B Models**: 32+ CPU cores, 64GB RAM
- **70B Models**: 64+ CPU cores, 128GB+ RAM

### Parameters

Configure model parameters to optimize performance:

- **Temperature**: Controls randomness (0.0-1.0)
- **Max Tokens**: Maximum response length
- **Top P**: Nucleus sampling parameter
- **Top K**: Top-k sampling parameter
- **Stop Sequences**: Custom stop sequences

## Infrastructure Setup

### Docker Deployment

Use Docker for easy deployment and management:

```dockerfile
FROM ollama/ollama:latest

# Copy model files
COPY models/ /root/.ollama/models/

# Expose port
EXPOSE 11434

# Start Ollama
CMD ["ollama", "serve"]
```

### Kubernetes Deployment

For production deployments, use Kubernetes:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ollama-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ollama
  template:
    metadata:
      labels:
        app: ollama
    spec:
      containers:
      - name: ollama
        image: ollama/ollama:latest
        ports:
        - containerPort: 11434
        resources:
          requests:
            memory: "32Gi"
            cpu: "8"
          limits:
            memory: "64Gi"
            cpu: "16"
```

### Load Balancing

For high availability, implement load balancing:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: ollama-service
spec:
  selector:
    app: ollama
  ports:
  - port: 11434
    targetPort: 11434
  type: LoadBalancer
```

## Security Configuration

### Network Security

Implement proper network security:

1. **Firewall Rules**: Restrict access to inference servers
2. **VPN Access**: Use VPN for secure remote access
3. **SSL/TLS**: Enable HTTPS for all communications
4. **Authentication**: Implement API key or token-based authentication

### Data Security

Ensure data security and privacy:

1. **Encryption**: Encrypt data in transit and at rest
2. **Access Controls**: Implement role-based access controls
3. **Audit Logging**: Log all model interactions
4. **Data Retention**: Define data retention policies

## Performance Optimization

### Model Optimization

Optimize models for better performance:

1. **Quantization**: Use quantized models to reduce memory usage
2. **Model Pruning**: Remove unnecessary parameters
3. **Batch Processing**: Process multiple requests together
4. **Caching**: Cache frequently requested responses

### Infrastructure Optimization

Optimize your infrastructure:

1. **GPU Optimization**: Use appropriate GPU drivers and CUDA versions
2. **Memory Management**: Optimize memory allocation and usage
3. **Network Optimization**: Use high-speed network connections
4. **Storage Optimization**: Use fast storage for model files

## Monitoring and Maintenance

### Health Monitoring

Monitor model health and performance:

```python
import requests
import time

def check_model_health(url):
    try:
        response = requests.get(f"{url}/health")
        return response.status_code == 200
    except:
        return False

def monitor_model():
    while True:
        if not check_model_health("http://localhost:11434"):
            print("Model is down, restarting...")
            restart_model()
        time.sleep(60)
```

### Resource Monitoring

Monitor resource usage:

- **GPU Utilization**: Track GPU memory and compute usage
- **CPU Usage**: Monitor CPU utilization and load
- **Memory Usage**: Track RAM usage and swap activity
- **Network Usage**: Monitor network bandwidth and latency

### Logging

Implement comprehensive logging:

```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('model.log'),
        logging.StreamHandler()
    ]
)
```

## Troubleshooting

### Common Issues

**Model Not Loading**
- Check if model files are present and accessible
- Verify sufficient disk space for model files
- Check model file integrity and permissions

**Out of Memory Errors**
- Reduce model size or use quantization
- Increase available RAM or VRAM
- Implement model offloading strategies

**Slow Performance**
- Check GPU drivers and CUDA installation
- Optimize model parameters and batch size
- Monitor system resources for bottlenecks

**Network Connectivity**
- Verify firewall rules and network configuration
- Check DNS resolution and routing
- Test connectivity with simple HTTP requests

### Debugging Tools

Use debugging tools to diagnose issues:

```bash
# Check GPU status
nvidia-smi

# Monitor system resources
htop

# Check network connectivity
curl -v http://localhost:11434/health

# View logs
tail -f /var/log/ollama.log
```

## Integration with Onyx

### Chat Interface

Configure self-hosted models for use in Onyx's chat interface:

1. Set as default model for general conversations
2. Configure model-specific parameters
3. Enable model switching for users
4. Set up fallback options

### Agent Configuration

Use self-hosted models with Onyx agents:

1. Assign specific models to agents
2. Configure custom instructions for each agent
3. Set up model-specific prompts and templates
4. Enable streaming responses where supported

### Document Processing

Leverage self-hosted models for document analysis:

1. Configure for document summarization
2. Enable question answering from documents
3. Set up content extraction and analysis
4. Use embedding models for semantic search

## Best Practices

### Deployment Best Practices

- **Start Small**: Begin with smaller models and scale up
- **Test Thoroughly**: Test models with real-world data
- **Monitor Closely**: Implement comprehensive monitoring
- **Plan for Scale**: Design infrastructure for future growth

### Security Best Practices

- **Principle of Least Privilege**: Grant minimal necessary permissions
- **Regular Updates**: Keep models and infrastructure updated
- **Backup Strategy**: Implement regular backups of model configurations
- **Incident Response**: Have a plan for security incidents

### Performance Best Practices

- **Load Testing**: Test models under expected load
- **Optimization**: Continuously optimize model and infrastructure
- **Caching**: Implement appropriate caching strategies
- **Monitoring**: Monitor performance metrics continuously

## Next Steps

After configuring self-hosted LLMs in Onyx:

1. **Test the Integration**: Verify that models are working correctly
2. **Monitor Performance**: Track response times and resource usage
3. **Optimize Configuration**: Fine-tune parameters for your use cases
4. **Train Users**: Educate users on model capabilities and limitations
5. **Scale Deployment**: Gradually increase usage while monitoring performance

For additional support or questions about self-hosted LLM integration, refer to the documentation for your chosen inference server or contact the Onyx support team. 