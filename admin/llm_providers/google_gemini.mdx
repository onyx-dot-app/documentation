---
title: "Google Gemini"
description: "Configure Google Gemini models for use with Onyx"
icon: "google"
---

# Google Gemini Configuration

Google Gemini is Google's latest family of multimodal AI models, offering powerful capabilities for text generation,
reasoning, and multimodal tasks. This guide will help you configure Gemini models in Onyx.

## Overview

Google Gemini models are available through Google AI Studio and Google Cloud Vertex AI.
Onyx supports integration with both platforms,
allowing you to leverage Gemini's advanced capabilities for your AI applications.

### Available Models

- **Gemini Pro**: Optimized for text generation and reasoning tasks
- **Gemini Flash**: Fast and efficient model for real-time applications
- **Gemini Pro Vision**: Multimodal model supporting text and image inputs
- **Gemini Ultra**: Most capable model for complex reasoning tasks

## Prerequisites

Before configuring Google Gemini in Onyx, ensure you have:

- **Google Cloud Project**: A Google Cloud project with billing enabled
- **API Access**: Access to Google AI Studio or Vertex AI
- **Authentication**: Proper authentication credentials (API key or service account)

## Configuration Options

### Option 1: Google AI Studio (Recommended)

Google AI Studio provides a simple API key-based authentication method.

#### Step 1: Get API Key

<Steps>
  <Step title="Open Google AI Studio">
    Visit [Google AI Studio](https://makersuite.google.com/app/apikey)
  </Step>

  <Step title="Sign in">
    Sign in with your Google account
  </Step>

  <Step title="Create API key">
    Click "Create API Key"
  </Step>

  <Step title="Copy API key">
    Copy the generated API key
  </Step>
</Steps>

#### Step 2: Configure in Onyx

<Steps>
  <Step title="Open LLM config">
    Navigate to the Onyx admin panel
  </Step>

  <Step title="Select Google Gemini">
    Go to **LLM Config** > **Google Gemini**
  </Step>

  <Step title="Choose AI Studio auth">
    Select **Google AI Studio** as the authentication method
  </Step>

  <Step title="Enter API key">
    Enter your API key
  </Step>

  <Step title="Choose model">
    Choose your preferred model (Gemini Pro, Gemini Flash, etc.)
  </Step>

  <Step title="Adjust parameters">
    Configure additional parameters as needed
  </Step>
</Steps>

### Option 2: Google Cloud Vertex AI

For enterprise deployments, use Vertex AI for enhanced security and management features.

#### Step 1: Enable Vertex AI API

<Steps>
  <Step title="Open Cloud Console">
    Go to the [Google Cloud Console](https://console.cloud.google.com/)
  </Step>

  <Step title="Select project">
    Select your project
  </Step>

  <Step title="Open API Library">
    Navigate to **APIs & Services** > **Library**
  </Step>

  <Step title="Find Vertex AI API">
    Search for "Vertex AI API"
  </Step>

  <Step title="Enable API">
    Click **Enable**
  </Step>
</Steps>

#### Step 2: Create Service Account

<Steps>
  <Step title="Open Service Accounts">
    Go to **IAM & Admin** > **Service Accounts**
  </Step>

  <Step title="Create account">
    Click **Create Service Account**
  </Step>

  <Step title="Name account">
    Provide a name and description
  </Step>

  <Step title="Assign role">
    Assign the **Vertex AI User** role
  </Step>

  <Step title="Download key">
    Create and download the JSON key file
  </Step>
</Steps>

#### Step 3: Configure in Onyx

<Steps>
  <Step title="Open LLM config">
    Navigate to the Onyx admin panel
  </Step>

  <Step title="Select Google Gemini">
    Go to **LLM Config** > **Google Gemini**
  </Step>

  <Step title="Choose Vertex AI auth">
    Select **Vertex AI** as the authentication method
  </Step>

  <Step title="Upload key">
    Upload your service account JSON key
  </Step>

  <Step title="Enter project ID">
    Enter your Google Cloud project ID
  </Step>

  <Step title="Choose model and region">
    Choose your preferred model and region
  </Step>
</Steps>

## Model Configuration

### Model Selection

Choose the appropriate Gemini model based on your use case:

- **Gemini Pro**: General-purpose text generation and reasoning
- **Gemini Flash**: Fast responses for real-time applications
- **Gemini Pro Vision**: Tasks requiring image and text understanding
- **Gemini Ultra**: Complex reasoning and analysis tasks

### Parameters

Configure model parameters to optimize performance:

- **Temperature**: Controls randomness (0.0-1.0)
- **Max Tokens**: Maximum response length
- **Top P**: Nucleus sampling parameter
- **Top K**: Top-k sampling parameter

### Safety Settings

Gemini includes built-in safety filters that can be configured:

- **Harassment**: Filter for harassing content
- **Hate Speech**: Filter for hate speech
- **Sexually Explicit**: Filter for explicit content
- **Dangerous Content**: Filter for dangerous activities

## Usage Examples

### Basic Text Generation

```python
# Example API call to Gemini Pro
{
  "model": "gemini-pro",
  "prompt": "Explain quantum computing in simple terms",
  "temperature": 0.7,
  "max_tokens": 500
}
```

### Multimodal Input

```python
# Example with image and text
{
  "model": "gemini-pro-vision",
  "prompt": "Describe what you see in this image",
  "image": "base64_encoded_image",
  "temperature": 0.3
}
```

## Best Practices

### Performance Optimization

- **Model Selection**: Choose the right model for your specific task
- **Prompt Engineering**: Craft clear, specific prompts for better results
- **Parameter Tuning**: Adjust temperature and other parameters based on use case
- **Caching**: Implement response caching for frequently asked questions

### Cost Management

- **Token Usage**: Monitor token consumption to manage costs
- **Model Efficiency**: Use smaller models for simple tasks
- **Batch Processing**: Group similar requests when possible
- **Rate Limiting**: Implement appropriate rate limits

### Security Considerations

- **API Key Security**: Store API keys securely and rotate regularly
- **Input Validation**: Validate all inputs before sending to the API
- **Output Filtering**: Implement additional content filtering if needed
- **Audit Logging**: Log all API interactions for compliance

## Troubleshooting

### Common Issues

**Authentication Errors**
- Verify API key is correct and has proper permissions
- Check if the API key is enabled for the correct services
- Ensure billing is enabled for your Google Cloud project

**Rate Limiting**
- Implement exponential backoff for retries
- Monitor API quotas and usage limits
- Consider using multiple API keys for load distribution

**Model Availability**
- Check if the selected model is available in your region
- Verify model access permissions
- Monitor Google's service status for outages

### Error Handling

Implement proper error handling for common scenarios:

```python
try:
    response = gemini_api.generate_text(prompt)
except RateLimitError:
    # Implement exponential backoff
    time.sleep(backoff_delay)
except AuthenticationError:
    # Check API key and permissions
    verify_credentials()
except ModelError:
    # Handle model-specific errors
    fallback_to_alternative_model()
```

## Monitoring and Analytics

### Key Metrics

Monitor these metrics to ensure optimal performance:

- **Response Time**: Average time to receive responses
- **Success Rate**: Percentage of successful API calls
- **Token Usage**: Total tokens consumed
- **Cost per Request**: Average cost per API call
- **Error Rate**: Percentage of failed requests

### Logging

Enable comprehensive logging for debugging and compliance:

- **Request Logs**: Log all API requests and responses
- **Error Logs**: Capture detailed error information
- **Usage Logs**: Track token usage and costs
- **Performance Logs**: Monitor response times and throughput

## Integration with Onyx

### Chat Interface

Configure Gemini models for use in Onyx's chat interface:

- Set as default model for general conversations
- Configure model-specific parameters
- Enable model switching for users
- Set up fallback options

### Agent Configuration

Use Gemini models with Onyx agents:

- Assign Gemini models to specific agents
- Configure custom instructions for each agent
- Set up model-specific prompts and templates
- Enable multimodal capabilities where needed

### Document Processing

Leverage Gemini's capabilities for document analysis:

- Configure for document summarization
- Enable question answering from documents
- Set up content extraction and analysis
- Enable multilingual document processing

## Next Steps

After configuring Google Gemini in Onyx:

- **Test the Integration**: Verify that the model is working correctly
- **Monitor Performance**: Track response times and success rates
- **Optimize Parameters**: Fine-tune model parameters for your use cases
- **Train Users**: Educate users on Gemini's capabilities and limitations
- **Scale Usage**: Gradually increase usage while monitoring costs and performance

For additional support or questions about Google Gemini integration,
refer to the [Google AI documentation](https://ai.google.dev/) or contact the Onyx support team.
