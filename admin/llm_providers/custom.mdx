---
title: Custom Model Servers
description: "Configure custom OpenAI-compatible model servers for Onyx"
icon: "code"
---

Configure Onyx to work with custom model servers and OpenAI-compatible APIs. This flexibility allows you to integrate with specialized deployments, custom fine-tuned models, or any service that implements the OpenAI API specification.

## What are Custom Model Servers?

Custom model servers are self-hosted or third-party services that provide language model APIs compatible with the OpenAI API format. This includes:

- **Self-hosted model servers** using frameworks like FastAPI, vLLM, or Text Generation Inference
- **Custom fine-tuned models** deployed on your infrastructure
- **Alternative API providers** that implement OpenAI-compatible endpoints
- **Specialized model deployments** for specific use cases or domains

## Supported Frameworks

### **Popular Model Serving Frameworks**

#### **vLLM**
- High-performance serving for large language models
- Excellent GPU utilization and throughput
- OpenAI-compatible API out of the box
- Supports many popular models

#### **Text Generation Inference (TGI)**
- Hugging Face's optimized inference server
- Production-ready with batching and streaming
- Support for various quantization techniques
- Docker-ready deployment

#### **FastChat**
- Multi-model serving platform
- OpenAI-compatible API
- Support for conversation templates
- Easy model deployment

#### **Triton Inference Server**
- NVIDIA's production inference server
- High-performance serving
- Multi-framework support
- Advanced optimization features

### **Cloud Alternatives**
- **Hugging Face Inference API**: Hosted inference endpoints
- **Together AI**: Hosted open-source models
- **Anyscale Endpoints**: Ray-based model serving
- **RunPod Serverless**: GPU-based inference endpoints

## Basic Configuration

### **OpenAI-Compatible Setup**

Most custom model servers implement the OpenAI API specification, making integration straightforward:

1. **Access Admin Dashboard**: Navigate to your Onyx admin panel
2. **Add Custom LLM Provider**: Select "Custom LLM Provider"
3. **Configure Basic Settings**:

```
Provider Name: My Custom Server
API Base: http://your-server:port/v1
API Key: your-api-key (if required)
Model: your-model-name
```

### **Required Endpoints**

For full compatibility, your custom server should implement these endpoints:

- **POST /v1/chat/completions**: Chat-based completions (recommended)
- **POST /v1/completions**: Text completions (optional)
- **GET /v1/models**: List available models (optional)

### **Request Format**

Your server should accept requests in OpenAI format:

```json
{
  "model": "your-model-name",
  "messages": [
    {"role": "user", "content": "Hello!"}
  ],
  "temperature": 0.7,
  "max_tokens": 150
}
```

## Example Implementations

### **vLLM Server Setup**

#### **Installation**
```bash
pip install vllm
```

#### **Start Server**
```bash
python -m vllm.entrypoints.openai.api_server \
  --model microsoft/DialoGPT-medium \
  --host 0.0.0.0 \
  --port 8000
```

#### **Docker Deployment**
```bash
docker run --gpus all \
  -v ~/.cache/huggingface:/root/.cache/huggingface \
  -p 8000:8000 \
  vllm/vllm-openai:latest \
  --model microsoft/DialoGPT-medium \
  --host 0.0.0.0
```

#### **Onyx Configuration**
```
API Base: http://localhost:8000/v1
Model: microsoft/DialoGPT-medium
```

### **Text Generation Inference**

#### **Docker Setup**
```bash
docker run --gpus all --shm-size 1g \
  -p 8080:80 \
  -v $PWD/data:/data \
  ghcr.io/huggingface/text-generation-inference:latest \
  --model-id microsoft/DialoGPT-medium \
  --num-shard 1
```

#### **Onyx Configuration**
```
API Base: http://localhost:8080/v1
Model: microsoft/DialoGPT-medium
```

### **FastAPI Custom Server**

Here's a basic example of creating your own OpenAI-compatible server:

#### **Server Implementation**
```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Optional
import uvicorn

app = FastAPI()

class Message(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    model: str
    messages: List[Message]
    temperature: Optional[float] = 0.7
    max_tokens: Optional[int] = 150

class ChatResponse(BaseModel):
    id: str
    object: str = "chat.completion"
    choices: List[dict]
    usage: dict

@app.post("/v1/chat/completions")
async def chat_completions(request: ChatRequest):
    # Your model inference logic here
    response_text = f"Response to: {request.messages[-1].content}"
    
    return ChatResponse(
        id="custom-response-id",
        choices=[{
            "index": 0,
            "message": {
                "role": "assistant",
                "content": response_text
            },
            "finish_reason": "stop"
        }],
        usage={
            "prompt_tokens": 10,
            "completion_tokens": 5,
            "total_tokens": 15
        }
    )

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

#### **Run Server**
```bash
python custom_server.py
```

#### **Onyx Configuration**
```
API Base: http://localhost:8000/v1
Model: custom-model
```

## Advanced Configuration

### **Authentication**

#### **API Key Authentication**
Most custom servers support API key authentication:

```python
headers = {
    "Authorization": f"Bearer {api_key}",
    "Content-Type": "application/json"
}
```

Configure in Onyx:
```
API Key: your-secret-api-key
```

#### **Custom Headers**
For servers requiring custom headers, you may need to modify the Onyx integration.

### **Model Parameters**

Configure model behavior through request parameters:

```json
{
  "temperature": 0.7,
  "top_p": 0.9,
  "top_k": 50,
  "max_tokens": 2048,
  "stop": ["\n\n", "Human:", "Assistant:"],
  "stream": false
}
```

### **Response Streaming**

For real-time responses, implement streaming:

```python
@app.post("/v1/chat/completions")
async def chat_completions(request: ChatRequest):
    if request.stream:
        return StreamingResponse(
            generate_stream_response(request),
            media_type="text/plain"
        )
    # Non-streaming response
```

## Extending Onyx Compatibility

### **Custom LLM Interface**

For non-standard APIs, you can extend Onyx's LLM interface. Modify the [custom LLM implementation](https://github.com/onyx-dot-app/onyx/blob/main/backend/onyx/llm/custom_llm.py#L14) in the Onyx codebase.

#### **Basic Extension Example**

```python
from onyx.llm.interfaces import LLM

class CustomLLMProvider(LLM):
    def __init__(self, api_base: str, api_key: str, model_name: str):
        self.api_base = api_base
        self.api_key = api_key
        self.model_name = model_name
    
    def invoke(self, prompt: str) -> str:
        # Custom API call implementation
        response = requests.post(
            f"{self.api_base}/custom/endpoint",
            headers={"Authorization": f"Bearer {self.api_key}"},
            json={
                "prompt": prompt,
                "model": self.model_name
            }
        )
        return response.json()["text"]
```

#### **Rebuilding Onyx**

After modifying the LLM interface:

1. **Update Code**: Modify the custom LLM implementation
2. **Rebuild Container**: Rebuild the Onyx backend container
3. **Deploy**: Deploy the updated version
4. **Test**: Verify integration works correctly

### **Request/Response Transformation**

For APIs with different formats, implement transformation layers:

```python
def transform_request(onyx_request):
    """Transform Onyx request to custom format"""
    return {
        "input_text": onyx_request["messages"][-1]["content"],
        "parameters": {
            "temperature": onyx_request.get("temperature", 0.7),
            "max_length": onyx_request.get("max_tokens", 150)
        }
    }

def transform_response(custom_response):
    """Transform custom response to OpenAI format"""
    return {
        "choices": [{
            "message": {
                "role": "assistant",
                "content": custom_response["generated_text"]
            },
            "finish_reason": "stop"
        }]
    }
```

## Production Deployment

### **Load Balancing**

For high availability, deploy multiple model server instances:

#### **Nginx Configuration**
```nginx
upstream model_servers {
    server model-server-1:8000;
    server model-server-2:8000;
    server model-server-3:8000;
}

server {
    listen 80;
    
    location / {
        proxy_pass http://model_servers;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}
```

### **Docker Compose Setup**

```yaml
version: '3.8'
services:
  model-server-1:
    image: your-model-server:latest
    ports:
      - "8001:8000"
    environment:
      - MODEL_NAME=your-model
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  model-server-2:
    image: your-model-server:latest
    ports:
      - "8002:8000"
    environment:
      - MODEL_NAME=your-model
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  load-balancer:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - model-server-1
      - model-server-2
```

### **Monitoring and Health Checks**

#### **Health Endpoint**
Implement health checks in your server:

```python
@app.get("/health")
async def health_check():
    return {"status": "healthy", "model_loaded": True}
```

#### **Prometheus Metrics**
Add metrics collection:

```python
from prometheus_client import Counter, Histogram, generate_latest

request_count = Counter('model_requests_total', 'Total model requests')
request_duration = Histogram('model_request_duration_seconds', 'Request duration')

@app.middleware("http")
async def add_metrics(request: Request, call_next):
    start_time = time.time()
    response = await call_next(request)
    request_duration.observe(time.time() - start_time)
    request_count.inc()
    return response

@app.get("/metrics")
async def metrics():
    return Response(generate_latest(), media_type="text/plain")
```

## Security Considerations

### **Authentication and Authorization**
- **API Keys**: Use secure API keys for authentication
- **Token Validation**: Validate tokens on each request
- **Rate Limiting**: Implement rate limiting to prevent abuse
- **IP Whitelisting**: Restrict access to known IP addresses

### **Network Security**
- **HTTPS**: Always use HTTPS in production
- **Firewall Rules**: Configure appropriate firewall rules
- **VPN/Private Networks**: Use private networks when possible
- **Reverse Proxy**: Use reverse proxy for additional security

### **Model Security**
- **Model Isolation**: Isolate different model instances
- **Resource Limits**: Set appropriate resource limits
- **Input Validation**: Validate all input parameters
- **Output Filtering**: Filter potentially harmful outputs

## Performance Optimization

### **Caching**
Implement response caching for repeated queries:

```python
import redis
import json

redis_client = redis.Redis(host='localhost', port=6379)

@app.post("/v1/chat/completions")
async def chat_completions(request: ChatRequest):
    cache_key = f"model:{request.model}:messages:{hash(str(request.messages))}"
    
    # Check cache
    cached_response = redis_client.get(cache_key)
    if cached_response:
        return json.loads(cached_response)
    
    # Generate response
    response = generate_response(request)
    
    # Cache response
    redis_client.setex(cache_key, 3600, json.dumps(response))
    
    return response
```

### **Batching**
Implement request batching for better throughput:

```python
from asyncio import Queue, gather
import asyncio

batch_queue = Queue()
batch_size = 8
batch_timeout = 0.1

async def batch_processor():
    while True:
        batch = []
        try:
            # Collect requests for batching
            for _ in range(batch_size):
                request = await asyncio.wait_for(
                    batch_queue.get(), 
                    timeout=batch_timeout
                )
                batch.append(request)
        except asyncio.TimeoutError:
            pass
        
        if batch:
            # Process batch
            results = await process_batch(batch)
            # Return results to waiting requests
```

### **Resource Management**
Monitor and manage resources:

```python
import psutil
import GPUtil

@app.get("/status")
async def server_status():
    return {
        "cpu_percent": psutil.cpu_percent(),
        "memory_percent": psutil.virtual_memory().percent,
        "gpu_utilization": [gpu.load for gpu in GPUtil.getGPUs()],
        "active_requests": active_request_count
    }
```

## Troubleshooting

### **Common Issues**

#### **Connection Errors**
```
Connection refused or timeout
```
**Solutions:**
- Verify server is running and accessible
- Check firewall and network configuration
- Ensure correct host and port in configuration
- Test with curl or similar tools

#### **Authentication Errors**
```
401 Unauthorized or 403 Forbidden
```
**Solutions:**
- Verify API key is correct
- Check authentication method implementation
- Ensure proper headers are being sent
- Review server-side authentication logic

#### **Model Loading Errors**
```
Model not found or failed to load
```
**Solutions:**
- Verify model path and availability
- Check model format compatibility
- Ensure sufficient memory and storage
- Review server logs for detailed errors

### **Debugging Tools**

#### **Request Logging**
```python
import logging

logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

@app.middleware("http")
async def log_requests(request: Request, call_next):
    logger.debug(f"Request: {request.method} {request.url}")
    logger.debug(f"Headers: {request.headers}")
    
    response = await call_next(request)
    
    logger.debug(f"Response: {response.status_code}")
    return response
```

#### **Performance Profiling**
```python
import cProfile
import pstats

@app.post("/v1/chat/completions")
async def chat_completions(request: ChatRequest):
    profiler = cProfile.Profile()
    profiler.enable()
    
    result = await generate_response(request)
    
    profiler.disable()
    stats = pstats.Stats(profiler)
    stats.sort_stats('cumulative')
    stats.print_stats()
    
    return result
```

## Use Cases

### **When to Use Custom Model Servers**

**Ideal Scenarios:**
- **Specialized Models**: Custom fine-tuned models for specific domains
- **Cost Optimization**: Self-hosted models to reduce API costs
- **Data Privacy**: Keep all processing on-premises
- **Custom Logic**: Special pre/post-processing requirements
- **Performance**: Optimized deployments for specific hardware

**Examples:**
- **Legal AI**: Fine-tuned models for legal document analysis
- **Medical AI**: Healthcare-specific language models
- **Code Generation**: Custom code models for specific frameworks
- **Multilingual**: Models optimized for specific languages
- **Domain-Specific**: Models trained on proprietary data

### **Integration Patterns**

#### **Hybrid Deployment**
Use custom models alongside cloud providers:
- Custom models for specialized tasks
- Cloud models for general queries
- Fallback between providers

#### **Multi-Tenant Serving**
Serve different models for different users:
- Organization-specific fine-tuned models
- Role-based model selection
- A/B testing different model versions

## Getting Help

For custom model server support:
- **Framework Documentation**: Refer to your chosen framework's docs
- **Community Forums**: Framework-specific communities and Discord servers
- **GitHub Issues**: Framework repositories for bug reports and features

For Onyx integration help:
- **Onyx Slack**: Join our [community](https://join.slack.com/t/onyx-dot-app/shared_invite/zt-34lu4m7xg-TsKGO6h8PDvR5W27zTdyhA)
- **Documentation**: Return to [LLM Overview](../getting_started/llm)
- **GitHub Issues**: Report integration problems on [GitHub](https://github.com/onyx-dot-app/onyx)
- **Contributions**: Contribute improvements to Onyx's custom LLM interface

Custom model servers provide the ultimate flexibility for organizations with specialized requirements, allowing you to maintain full control over your AI infrastructure while leveraging Onyx's comprehensive knowledge management capabilities. 