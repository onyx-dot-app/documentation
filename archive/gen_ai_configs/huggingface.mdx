---
title: "HuggingFace Inference API"
description: "Configure Onyx to use HuggingFace APIs"
---

Refer to [Model Configs](https://docs.onyx.app/gen_ai_configs/overview#model-configs)
for how to set the environment variables for your particular deployment.

To use the HuggingFace Inference APIs, you must sign up for a `Pro Account` to get an API Key

1. After signing up for `Pro Account`, go to your user settings:

![HFSettings](/archive/images/gen_ai/HFSettings.png)

2. Copy the `User Access Token`

![HFAccessToken](/archive/images/gen_ai/HFAccessToken.png)

## Set Onyx to use `Llama-2-70B` via next-token generation prompting

On the `LLM` page in the Admin Panel add a `Custom LLM Provider` with the following settings:

![HFCustomLLMProvider1](/archive/images/gen_ai/HFCustomLLMProvider1.png)

![HFCustomLLMProvider2](/archive/images/gen_ai/HFCustomLLMProvider2.png)

**Update (November 2023)**: HuggingFace has stopped supporting very large models (>10GB) via the _Pro Plan_.
The latest options are to rent dedicated hardware from them via Inference Endpoint or get an Enterprise Plan.
The _Pro Plan_ still supports smaller models but these produce worse results for Onyx.
